{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential removal of links and resiliency testing\n",
    "\n",
    "- Disruption of a network by removal of links, based on:\n",
    "    + Sum of betweenness centrality of from and to nodes\n",
    "    + Volume of commodity flow\n",
    "- Calculation of performance in terms of cost and unmet demand by re-running disrupted network. \n",
    "- Plot link removal along x-axis and performance on y-axis, comparing networks of differing evenness. Dynamic report generated in an RMarkdown automatically from this Notebook.\n",
    "\n",
    "**Assumptions**\n",
    "\n",
    "- Working in a Python 3.x environment for this notebook\n",
    "    + Refer to the README in this repository for instructions on setup of all dependencies with `conda`\n",
    "- Access to ArcGIS license server if necessary\n",
    "- FTOT scenario was run with Network Density Reduction (NDR) off\n",
    "    + NDR_On should be False in the scenario XML file\n",
    "\n",
    "*Reference*\n",
    "\n",
    "- [NetworkX Documentation](https://networkx.github.io/documentation/stable/tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sqlalchemy\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "import momepy # for conversion from geopandas GeoDataFrame to networkX Graph\n",
    "import subprocess\n",
    "import shutil\n",
    "import webbrowser\n",
    "import resiliency_disruptions\n",
    "\n",
    "# Uses Reference Scenario 7 as an example. Modify `scen_name` and `scen_path` for your scenario.\n",
    "scen_name = 'rs7_capacity'\n",
    "scen_path = r'C:\\FTOT\\scenarios\\reference_scenarios\\rs7_capacity'\n",
    "\n",
    "# Sets whether the base scenario has background flows\n",
    "BACKGROUND_FLOWS = pd.isna(nx_edges['volume']).any()\n",
    "\n",
    "# Set the variable `DO_VOLUME` to `True` to use background vehicle flows as measure of importance\n",
    "# If DO_VOLUME is False or if BACKGROUND_FLOWS above is False, betweeness centrality will be used for rankings\n",
    "DO_VOLUME = False\n",
    "\n",
    "# TODO: use GDB instead for the road network layer\n",
    "shp_path = os.path.join(scen_path, 'temp_networkx_shp_files')\n",
    "\n",
    "picklename = os.path.join(scen_path, 'BetweenessG.pickle')\n",
    "\n",
    "\n",
    "# TODO: may not require this anymore though another code change is decreasing buffer from 100 to 50 miles\n",
    "if not os.path.exists(shp_path):\n",
    "    print('Please modify the FTOT code using the `ftot_networkx.py` and `ftot_routing.py` scripts in this repository and run the scenario again.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in prepared betweeness centrality and road network graph data\n",
    "# If these don't exist, the following steps will create them\n",
    "if os.path.exists(picklename):\n",
    "    file = open(picklename, 'rb')\n",
    "    betweenness_dict_road = pickle.load(file)\n",
    "    G_road = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start by using betweenness centrality calculation using networkX\n",
    "if not os.path.exists(picklename):\n",
    "    # TODO: replace use of geopandas with ogr from osgeo which is already in this environment (see read_gdb method in FTOT as an example)\n",
    "    road = gpd.read_file(os.path.join(shp_path, 'road.shp'))\n",
    "    \n",
    "    # convert from geodataframe to Graph for networkX\n",
    "    G_road = momepy.gdf_to_nx(road, approach='primal')\n",
    "    \n",
    "    # Process the networkX graph\n",
    "    G_road = nx.convert_node_labels_to_integers(G_road, first_label=0, ordering='default', label_attribute=\"xy_coord_label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run betweenness centrality on the NetworkX graph\n",
    "# Note: This step might take several minutes to a few hours\n",
    "# Run if pickle not available\n",
    "# TODO: can likely skip this step if running 'V' not 'BC' but then do not have betweenness_dict_road variable\n",
    "if not os.path.exists(picklename):\n",
    "    print('Running Betweenness Centrality calculations. This might take more than 20 minutes.')\n",
    "    betweenness_dict_road = nx.betweenness_centrality(G_road, normalized=False, weight='Length')\n",
    "    print('Completed Betweenness Centrality calculations.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save with pickle\n",
    "# On load, need to know that there are two objects in this pickle, the betweenness centrality dict and the network G\n",
    "if not os.path.exists(picklename):\n",
    "    with open(picklename, 'wb') as handle:\n",
    "        pickle.dump(betweenness_dict_road, handle)\n",
    "        pickle.dump(G_road, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Join Betweenness Centrality calculations to edges\n",
    "\n",
    "- Sum BC for each node of a link\n",
    "- Create data frame for repeated link removal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\FTOT\\scenarios\\reference_scenarios\\rs7_capacity\n"
     ]
    }
   ],
   "source": [
    "# Read in FTOT data\n",
    "print(scen_path)\n",
    "db_name = 'main.db'\n",
    "\n",
    "db_path = 'sqlite:///' + os.path.join(scen_path, db_name)\n",
    "\n",
    "engine = sqlalchemy.create_engine(db_path)\n",
    "\n",
    "table_name = 'networkx_edges'\n",
    "nx_edges = pd.read_sql_table(table_name, engine)\n",
    "\n",
    "\n",
    "# Set whether scenario has background flow data\n",
    "# volume column in DB is filled with NULL if no\n",
    "background_flows = pd.isna(nx_edges['volume']).any()\n",
    "\n",
    "# TODO: bring in additional metrics (route_cost, transport_cost, route_cost_transport, co2_cost) from networkx_edge_costs table\n",
    "\n",
    "table_name = 'networkx_nodes'\n",
    "nx_nodes = pd.read_sql_table(table_name, engine)\n",
    "\n",
    "table_name = 'optimal_variables'\n",
    "optimal_vars = pd.read_sql_table(table_name, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: replace use of geopandas with ogr from osgeo which is already in this environment (see read_gdb method in FTOT as an example)\n",
    "road_orig_label = gpd.read_file(os.path.join(shp_path, 'road.shp'))\n",
    "# convert from geodataframe to Graph for networkX\n",
    "G_road_orig_label = momepy.gdf_to_nx(road_orig_label, approach='primal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "road_orig_label_nodes = list(G_road_orig_label.nodes) # these values are the shape_x and shape_y values in `networkx_nodes` \n",
    "# Use that to get node_id from networkx_edges in the database\n",
    "# Then use those id values to get edges info\n",
    "# Then line up the new integer labels with this list of ids to get betweenness centrality for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make the betweenness_centrality values as the framework to join in shape_x, shape_y, and node_id\n",
    "# TODO: can likely skip this step if using 'V' not 'BC'\n",
    "bc_df_road = pd.DataFrame.from_dict(betweenness_dict_road, orient = 'index')\n",
    "bc_df_road = bc_df_road.rename(columns = {0: 'BC'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "node_shape_df_road = pd.DataFrame(road_orig_label_nodes)\n",
    "\n",
    "# TODO: can likely skip this step if using 'V' not 'BC' but may complicate things if missing shape_x and shape_y\n",
    "bc_shape_df_road = pd.concat([bc_df_road, node_shape_df_road], axis = 1)\n",
    "bc_shape_df_road = bc_shape_df_road.rename(columns = {0: 'shape_x', 1: 'shape_y'})\n",
    "\n",
    "# Now add node_id from networkx_nodes, using pandas merge with left join\n",
    "# Use both shape_x and shape_y to identify the nodes correctly\n",
    "\n",
    "bc_node_df = pd.merge(bc_shape_df_road, nx_nodes, on = ['shape_x', 'shape_y'], how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now use this dataframe to populate a dataframe of edges\n",
    "# We will want the following from networkx_edges:\n",
    "# edge_id, from_node_id, to_node_id, mode_source, miles, mode_source_oid\n",
    "# Then using the node_id column in the new bc_node_df, add these:\n",
    "# from_node_BC, to_node_BC\n",
    "# and sum those for sum_node_BC\n",
    "# TODO: can likely skip this step if using 'V' not 'BC'\n",
    "merge_from = pd.merge(nx_edges, bc_node_df[['BC', 'node_id']],\n",
    "                      left_on = 'from_node_id',\n",
    "                      right_on = 'node_id',\n",
    "                      how = 'left')\n",
    "merge_from = merge_from.rename(columns = {'BC': 'from_node_BC'})\n",
    "\n",
    "merge_to = pd.merge(merge_from, bc_node_df[['BC', 'node_id']],\n",
    "                    left_on = 'to_node_id',\n",
    "                    right_on = 'node_id',\n",
    "                    how = 'left')\n",
    "merge_to = merge_to.rename(columns = {'BC': 'to_node_BC'})\n",
    "\n",
    "# Sum the BC values\n",
    "\n",
    "merge_to['sum_BC'] = merge_to.filter(like = \"node_BC\").sum(axis = 1)\n",
    "\n",
    "# Then from optimal_variables, get variable_name, nc_edge_id, mode, mode_oid, miles,\n",
    "# variable_value, converted_capacity, and converted_volume\n",
    "\n",
    "use_opt_vars = ['variable_type',\n",
    "               'var_id',\n",
    "               'variable_value',\n",
    "                'variable_name',\n",
    "                'nx_edge_id',\n",
    "                'mode_oid',\n",
    "                'converted_capacity',\n",
    "                'converted_volume'\n",
    "               ]\n",
    "\n",
    "merge_opt = pd.merge(merge_to, optimal_vars[use_opt_vars],\n",
    "                     left_on = 'edge_id',\n",
    "                     right_on = 'nx_edge_id',\n",
    "                     how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranked lists of edges to remove\n",
    "# First, keep only edges in the optimal solution\n",
    "# Then rank by sum_BC\n",
    "# Then just keep the columns we need, and reset the index\n",
    "# Note: in resiliency_disruptions.disrupt_network, the edges_remove DataFrame is sorted again by 'V' or 'BC'\n",
    "# TODO: update use_cols as needed for additional metrics\n",
    "use_cols = ['edge_id', 'from_node_id', 'to_node_id', 'length', 'capacity', 'volume', 'sum_BC',\n",
    "            'variable_type', 'variable_value', 'variable_name', 'nx_edge_id', 'mode_oid', 'converted_capacity',\n",
    "            'converted_volume']\n",
    "\n",
    "# TODO: if 'V', we may not have sum_BC column at this point\n",
    "# TODO: note that sort_values (both here and in resiliency_disruptions.disrupt_network) does not break ties (significant for 'V')\n",
    "edges_remove = merge_opt[merge_opt['variable_value'] > 0].sort_values(by = 'sum_BC', ascending = False).filter(items = use_cols).reset_index()\n",
    "\n",
    "edges_remove.to_csv(os.path.join(scen_path, 'Edges_to_Remove.csv'),\n",
    "                    index = False)\n",
    "\n",
    "edges_remove.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Scenarios, Disrupt, Run FTOT\n",
    "\n",
    "Create disrupted network by copying everything in `scen_path` to a new directory.\n",
    "\n",
    "Then overwrite the `networkx_edges` tables in that main.db with the disrupted versions.\n",
    "\n",
    "##### Assumptions:\n",
    "\n",
    "  1. ArcGIS with 64-bit geoprocessing is installed.\n",
    "  2. The FTOT version being used has been modified according to the `README` in this directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "disrupt_type = 'BC' # Can disrupt based on betweenness centrality, 'BC', or volume, 'V'\n",
    "disrupt_steps = 12  # This is the number of steps to use. Recommend at least 25.\n",
    "\n",
    "resiliency_disruptions.make_disruption_scenarios(disrupt_type, disrupt_steps, scen_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "resiliency_disruptions.disrupt_network(disrupt_type, disrupt_steps, scen_path, edges_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: move these higher up in the notebook\n",
    "PYTHON = r\"C:\\FTOT\\python3_env\\python.exe\"\n",
    "repo_location = %pwd\n",
    "repo_location = os.path.split(repo_location)[0]\n",
    "FTOT = r\"C:\\FTOT\\program\\ftot.py\"  # Optionally: os.path.join(repo_location, 'program', 'ftot.py')\n",
    "print(FTOT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Begin running O through M steps of FTOT on the disupted scenarios\n",
    "# This may take several hours, depending on size of the network and number of steps\n",
    "\n",
    "results = resiliency_disruptions.run_o_steps(disrupt_type, disrupt_steps, scen_path, PYTHON, FTOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Optional: Repeat with volume-based disruptions\n",
    "\n",
    "Creates a separate directory tree for the volume-based disruptions and carries out the disruption steps on that set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if BACKGROUND_FLOW AND DO_VOLUME:\n",
    "\n",
    "    disrupt_type = 'V'\n",
    "    disrupt_steps = 5\n",
    "\n",
    "    resiliency_disruptions.make_disruption_scenarios(disrupt_type, disrupt_steps, scen_path)\n",
    "    resiliency_disruptions.disrupt_network(disrupt_type, disrupt_steps, scen_path, edges_remove)\n",
    "    results = resiliency_disruptions.run_o_steps(disrupt_type, disrupt_steps, scen_path, PYTHON, FTOT)\n",
    "    results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate disruption result report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: bug in the handoff here, but not reported out to the user... looks like it succeeded but it did not\n",
    "# TODO: hand in the disrupt_type parameter as well (right now, hard-coded in RMarkdown as BC)\n",
    "R_process = subprocess.Popen(['Rscript.exe', 'compile_report.R', scen_path, BACKGROUND_FLOWS, DO_VOLUME,],\n",
    "                             stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "\n",
    "here = os.getcwd()\n",
    "webbrowser.open('file://' + os.path.realpath(os.path.join(here, 'Disruption_Results.html')))\n",
    "\n",
    "# TODO: move html file to scen_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.path.realpath(os.path.join(here, 'Disruption_Results.html'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
