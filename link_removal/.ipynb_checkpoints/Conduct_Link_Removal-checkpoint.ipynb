{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential removal of links and resiliency testing\n",
    "\n",
    "### Overview\n",
    "The notebook iteratively removes the links (edges) in the FTOT road network in order of importance to create distinct disruption scenarios and re-runs FTOT to determine the new optimal solutions and costs. Importance is measured by:\n",
    "\n",
    "- the sum of __betweenness centrality__ of a link's beginning and ending points __OR__   \n",
    "- the __volume__ of background freight flows on a link.\n",
    "\n",
    "The notebook outputs (i) CSV files with information on the links removed and scenario results and (ii) interactive report generated with RMarkdown. Note that scenario costs in the outputs are the minimized FTOT objective value, which is based on impeded transport cost, facility build costs, and unmet demand penalties.\n",
    "\n",
    "### Instructions\n",
    "_Before running this notebook,_ follow instructions the repository's README to (i) set up and activate the Python environment and (ii) run a baseline FTOT scenario.\n",
    "\n",
    "__(1) Update parameters__ in the cell labeled Step 1:  \n",
    "- Baseline scenario name and path  \n",
    "- Measure of importance (volume or betweeness centrality)  \n",
    "- Number of disruption steps  \n",
    "- True/False toggle to export maps for each disruption scenario  \n",
    "     \n",
    "__(2) Run all cells__ by going to the top menu bar > Cell > Run All.\n",
    "\n",
    "__(3) Review outputs__ in the folder with disruption scenarios:\n",
    "\n",
    "- Edges_to_Remove.csv - a list of edges to remove with their importance ranking\n",
    "- Results.csv - resulting scenario costs and other optimal variables after each disruption step\n",
    "- Disruption_Results.html - interactive summary report\n",
    "\n",
    "The disruption folder will be created in the same location as the baseline scenario folder. The new folder will have `V_disrupt` or `BC_disrupt` appended to the scenario name.\n",
    "\n",
    "This notebook may take several hours to run depending on the scenario size and number of disruption steps.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- You are working in a Python 3.x environment for this notebook. Refer to the README in this repository for setup instructions.\n",
    "- You have access to a ArcGIS license server if necessary\n",
    "- A baseline FTOT scenario was run with Network Density Reduction (NDR_On set to False in the scenario XML file) and with the swapped ftot_routing.py file from this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sqlalchemy\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "import momepy # for conversion from geopandas GeoDataFrame to networkX Graph\n",
    "import subprocess\n",
    "import shutil\n",
    "import webbrowser\n",
    "import resiliency_disruptions\n",
    "from osgeo import ogr\n",
    "\n",
    "PYTHON = r\"C:\\FTOT\\python3_env\\python.exe\"\n",
    "FTOT = r\"C:\\FTOT\\program\\ftot.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set User-Defined Parameters (USER INPUT REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Reference Scenario 7 as an example.\n",
    "# Modify `scen_name` and `scen_path` for your scenario.\n",
    "scen_name = 'rs7_capacity'\n",
    "scen_path = r'C:\\FTOT\\scenarios\\reference_scenarios\\rs7_capacity'\n",
    "\n",
    "# Enter disrupt_type 'BC' for betweeness centrality or 'V' for volume \n",
    "# Note: if background flows were not enabled in the baseline scenario,\n",
    "# the notebook will automatically switch to BC.\n",
    "disrupt_type = 'V'\n",
    "\n",
    "# Enter the number of disruption scenarios to generated\n",
    "# Recommend at least 25\n",
    "disrupt_steps = 25 \n",
    "\n",
    "# Set the variable `MAKE_MAPS` to `True` to output maps for each disruption scenario\n",
    "# Note this will increase runtime\n",
    "MAKE_MAPS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Importance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Betweenness Centrality calculations. This might take more than 20 minutes.\n",
      "Completed Betweenness Centrality calculations.\n"
     ]
    }
   ],
   "source": [
    "if disrupt_type == 'BC':\n",
    "    \n",
    "    # Read in prepared betweeness centrality and road network graph data\n",
    "    # If these don't exist, the following steps will create them\n",
    "    picklename = os.path.join(scen_path, 'BetweenessG.pickle')\n",
    "    if os.path.exists(picklename):\n",
    "        file = open(picklename, 'rb')\n",
    "        betweenness_dict_road = pickle.load(file)\n",
    "        G_road = pickle.load(file)\n",
    "    \n",
    "    # Run betweenness centrality on the NetworkX graph\n",
    "    # Note: This step might take several minutes to a few hours   \n",
    "    elif not os.path.exists(picklename):\n",
    "        G_road = resiliency_disruptions.read_gdb(os.path.join(scen_path, 'main.gdb'), 'road')\n",
    "        print('Running Betweenness Centrality calculations. This might take more than 20 minutes.')\n",
    "        betweenness_dict_road = nx.betweenness_centrality(G_road, normalized=False, weight='Length')\n",
    "        print('Completed Betweenness Centrality calculations.')\n",
    "        \n",
    "        # Save with pickle\n",
    "        # Upon load, this pickle will contain the network G_road and the betweenness centrality dict and \n",
    "        with open(picklename, 'wb') as handle:\n",
    "            pickle.dump(betweenness_dict_road, handle)\n",
    "            pickle.dump(G_road, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Associate Importance Metrics with Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in C:\\FTOT\\scenarios\\reference_scenarios\\rs7_capacity\n"
     ]
    }
   ],
   "source": [
    "# Read in FTOT data\n",
    "print('Reading in {}'.format(scen_path))\n",
    "\n",
    "db_name = 'main.db'\n",
    "db_path = 'sqlite:///' + os.path.join(scen_path, db_name)\n",
    "engine = sqlalchemy.create_engine(db_path)\n",
    "\n",
    "table_name = 'networkx_edges'\n",
    "nx_edges = pd.read_sql_table(table_name, engine)\n",
    "\n",
    "table_name = 'networkx_nodes'\n",
    "nx_nodes = pd.read_sql_table(table_name, engine)\n",
    "\n",
    "table_name = 'optimal_variables'\n",
    "optimal_vars = pd.read_sql_table(table_name, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background flows confirmed\n"
     ]
    }
   ],
   "source": [
    "# Check whether scenario has background flow data\n",
    "# volume column in DB is filled with NULL if no\n",
    "# Automatically revert to betweeness centrality if no background flow data\n",
    "# TODO: Confirm want to switch if ANY are null?\n",
    "BACKGROUND_FLOWS = pd.isna(nx_edges['volume']).any()\n",
    "if BACKGROUND_FLOWS:\n",
    "    print('Background flows confirmed')\n",
    "elif disrupt_type == 'V' and not BACKGROUND_FLOWS:\n",
    "    print('WARNING: Network does not have background flows.')\n",
    "    print('Switching importance measure to betweenness centrality.')\n",
    "    disrupt_type = 'BC'\n",
    "else:\n",
    "    print('Scenario does not have background flows. Proceeding with disrupt type BC.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disrupt_type == 'BC':\n",
    "    \n",
    "    # Get shape_x and shape_y\n",
    "    road_orig_label_nodes = list(G_road.nodes)\n",
    "    node_shape_df_road = pd.DataFrame(road_orig_label_nodes)\n",
    "    \n",
    "    # Make the betweenness_centrality values as the framework to join in shape_x, shape_y, and node_id\n",
    "    bc_df_road = pd.DataFrame.from_dict(betweenness_dict_road, orient = 'index')\n",
    "    bc_df_road = bc_df_road.rename(columns = {0: 'BC'}).reset_index()\n",
    "    \n",
    "    bc_shape_df_road = pd.concat([bc_df_road, node_shape_df_road], axis = 1)\n",
    "    bc_shape_df_road = bc_shape_df_road.rename(columns = {0: 'shape_x', 1: 'shape_y'})\n",
    "        \n",
    "    # Now add node_id from networkx_nodes, using pandas merge with left join\n",
    "    # Use both shape_x and shape_y to identify the nodes correctly\n",
    "    bc_node_df = pd.merge(bc_shape_df_road, nx_nodes, on = ['shape_x', 'shape_y'], how = 'left')\n",
    "\n",
    "    # Now use this dataframe to populate a dataframe of edges\n",
    "    # We will want the following from networkx_edges:\n",
    "    # edge_id, from_node_id, to_node_id, mode_source, miles, mode_source_oid\n",
    "    # Then using the node_id column in the new bc_node_df, add these:\n",
    "    # from_node_BC, to_node_BC\n",
    "    # and sum those for sum_node_BC\n",
    "    merge_from = pd.merge(nx_edges, bc_node_df[['BC', 'node_id']],\n",
    "                          left_on = 'from_node_id',\n",
    "                          right_on = 'node_id',\n",
    "                          how = 'left')\n",
    "    merge_from = merge_from.rename(columns = {'BC': 'from_node_BC'})\n",
    "\n",
    "    merge_to = pd.merge(merge_from, bc_node_df[['BC', 'node_id']],\n",
    "                        left_on = 'to_node_id',\n",
    "                        right_on = 'node_id',\n",
    "                        how = 'left')\n",
    "    merge_to = merge_to.rename(columns = {'BC': 'to_node_BC'})\n",
    "\n",
    "    # Sum the BC values\n",
    "    merge_to['sum_BC'] = merge_to.filter(like = \"node_BC\").sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disrupt_type == 'V':\n",
    "    merge_to = nx_edges.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge_id</th>\n",
       "      <th>from_node_id</th>\n",
       "      <th>to_node_id</th>\n",
       "      <th>artificial</th>\n",
       "      <th>mode_source</th>\n",
       "      <th>mode_source_oid</th>\n",
       "      <th>length</th>\n",
       "      <th>route_cost_scaling</th>\n",
       "      <th>capacity</th>\n",
       "      <th>volume</th>\n",
       "      <th>...</th>\n",
       "      <th>sum_BC</th>\n",
       "      <th>variable_type</th>\n",
       "      <th>var_id</th>\n",
       "      <th>variable_value</th>\n",
       "      <th>variable_name</th>\n",
       "      <th>nx_edge_id</th>\n",
       "      <th>mode_oid</th>\n",
       "      <th>converted_capacity</th>\n",
       "      <th>converted_volume</th>\n",
       "      <th>commodity_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>332</td>\n",
       "      <td>1887</td>\n",
       "      <td>2</td>\n",
       "      <td>road</td>\n",
       "      <td>282</td>\n",
       "      <td>3.541860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>399</td>\n",
       "      <td>1888</td>\n",
       "      <td>2</td>\n",
       "      <td>road</td>\n",
       "      <td>349</td>\n",
       "      <td>0.275609</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>84497.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>438</td>\n",
       "      <td>1889</td>\n",
       "      <td>2</td>\n",
       "      <td>road</td>\n",
       "      <td>388</td>\n",
       "      <td>0.087752</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>98053.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>448</td>\n",
       "      <td>1890</td>\n",
       "      <td>2</td>\n",
       "      <td>road</td>\n",
       "      <td>398</td>\n",
       "      <td>0.129394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>762652.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>452</td>\n",
       "      <td>1891</td>\n",
       "      <td>2</td>\n",
       "      <td>road</td>\n",
       "      <td>402</td>\n",
       "      <td>0.238547</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   edge_id  from_node_id  to_node_id  artificial mode_source  mode_source_oid  \\\n",
       "0        1           332        1887           2        road              282   \n",
       "1        2           399        1888           2        road              349   \n",
       "2        3           438        1889           2        road              388   \n",
       "3        4           448        1890           2        road              398   \n",
       "4        5           452        1891           2        road              402   \n",
       "\n",
       "     length  route_cost_scaling  capacity  volume  ...    sum_BC  \\\n",
       "0  3.541860                 1.0       NaN     NaN  ...       0.0   \n",
       "1  0.275609                 1.0       NaN     NaN  ...   84497.0   \n",
       "2  0.087752                 1.0       NaN     NaN  ...   98053.0   \n",
       "3  0.129394                 1.0       NaN     NaN  ...  762652.0   \n",
       "4  0.238547                 1.0       NaN     NaN  ...       0.0   \n",
       "\n",
       "   variable_type  var_id  variable_value  variable_name  nx_edge_id  mode_oid  \\\n",
       "0            NaN     NaN             NaN            NaN         NaN       NaN   \n",
       "1            NaN     NaN             NaN            NaN         NaN       NaN   \n",
       "2            NaN     NaN             NaN            NaN         NaN       NaN   \n",
       "3            NaN     NaN             NaN            NaN         NaN       NaN   \n",
       "4            NaN     NaN             NaN            NaN         NaN       NaN   \n",
       "\n",
       "   converted_capacity converted_volume  commodity_name  \n",
       "0                 NaN              NaN             NaN  \n",
       "1                 NaN              NaN             NaN  \n",
       "2                 NaN              NaN             NaN  \n",
       "3                 NaN              NaN             NaN  \n",
       "4                 NaN              NaN             NaN  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select optimal_vars DB columns to keep \n",
    "use_opt_vars = ['variable_type',\n",
    "               'var_id',\n",
    "               'variable_value',\n",
    "                'variable_name',\n",
    "                'nx_edge_id',\n",
    "                'mode_oid',\n",
    "                'converted_capacity',\n",
    "                'converted_volume',\n",
    "                'commodity_name'\n",
    "               ]\n",
    "\n",
    "merge_opt = pd.merge(merge_to, optimal_vars[use_opt_vars],\n",
    "                     left_on = 'edge_id',\n",
    "                     right_on = 'nx_edge_id',\n",
    "                     how = 'left')\n",
    "\n",
    "merge_opt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>edge_id</th>\n",
       "      <th>from_node_id</th>\n",
       "      <th>to_node_id</th>\n",
       "      <th>length</th>\n",
       "      <th>capacity</th>\n",
       "      <th>volume</th>\n",
       "      <th>sum_BC</th>\n",
       "      <th>variable_type</th>\n",
       "      <th>commodity_name</th>\n",
       "      <th>variable_value</th>\n",
       "      <th>variable_name</th>\n",
       "      <th>nx_edge_id</th>\n",
       "      <th>mode_oid</th>\n",
       "      <th>converted_capacity</th>\n",
       "      <th>converted_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14725</td>\n",
       "      <td>14726</td>\n",
       "      <td>8155</td>\n",
       "      <td>4140</td>\n",
       "      <td>0.056946</td>\n",
       "      <td>22500.000000</td>\n",
       "      <td>4941.0</td>\n",
       "      <td>4129353.0</td>\n",
       "      <td>Edge</td>\n",
       "      <td>freight_parcel</td>\n",
       "      <td>22679.618</td>\n",
       "      <td>Edge_29460</td>\n",
       "      <td>14726.0</td>\n",
       "      <td>269353.0</td>\n",
       "      <td>540000.000000</td>\n",
       "      <td>118584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5478</td>\n",
       "      <td>5479</td>\n",
       "      <td>4140</td>\n",
       "      <td>8155</td>\n",
       "      <td>0.056946</td>\n",
       "      <td>22500.000000</td>\n",
       "      <td>4941.0</td>\n",
       "      <td>4129353.0</td>\n",
       "      <td>Edge</td>\n",
       "      <td>freight_bulk</td>\n",
       "      <td>43134.521</td>\n",
       "      <td>Edge_10965</td>\n",
       "      <td>5479.0</td>\n",
       "      <td>269353.0</td>\n",
       "      <td>540000.000000</td>\n",
       "      <td>118584.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8669</td>\n",
       "      <td>8670</td>\n",
       "      <td>5525</td>\n",
       "      <td>6164</td>\n",
       "      <td>0.041683</td>\n",
       "      <td>23483.700275</td>\n",
       "      <td>8661.5</td>\n",
       "      <td>3374629.0</td>\n",
       "      <td>Edge</td>\n",
       "      <td>freight_parcel</td>\n",
       "      <td>58967.008</td>\n",
       "      <td>Edge_17348</td>\n",
       "      <td>8670.0</td>\n",
       "      <td>226482.0</td>\n",
       "      <td>563608.806599</td>\n",
       "      <td>207876.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10215</td>\n",
       "      <td>10216</td>\n",
       "      <td>6164</td>\n",
       "      <td>5525</td>\n",
       "      <td>0.041683</td>\n",
       "      <td>23483.700275</td>\n",
       "      <td>8661.5</td>\n",
       "      <td>3374629.0</td>\n",
       "      <td>Edge</td>\n",
       "      <td>freight_bulk</td>\n",
       "      <td>138302.430</td>\n",
       "      <td>Edge_20439</td>\n",
       "      <td>10216.0</td>\n",
       "      <td>226482.0</td>\n",
       "      <td>563608.806599</td>\n",
       "      <td>207876.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5481</td>\n",
       "      <td>5482</td>\n",
       "      <td>4141</td>\n",
       "      <td>4140</td>\n",
       "      <td>0.032872</td>\n",
       "      <td>13683.556054</td>\n",
       "      <td>5313.0</td>\n",
       "      <td>3078366.0</td>\n",
       "      <td>Edge</td>\n",
       "      <td>freight_bulk</td>\n",
       "      <td>43134.521</td>\n",
       "      <td>Edge_10971</td>\n",
       "      <td>5482.0</td>\n",
       "      <td>96162.0</td>\n",
       "      <td>328405.345291</td>\n",
       "      <td>127512.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  edge_id  from_node_id  to_node_id    length      capacity  volume  \\\n",
       "0  14725    14726          8155        4140  0.056946  22500.000000  4941.0   \n",
       "1   5478     5479          4140        8155  0.056946  22500.000000  4941.0   \n",
       "2   8669     8670          5525        6164  0.041683  23483.700275  8661.5   \n",
       "3  10215    10216          6164        5525  0.041683  23483.700275  8661.5   \n",
       "4   5481     5482          4141        4140  0.032872  13683.556054  5313.0   \n",
       "\n",
       "      sum_BC variable_type  commodity_name  variable_value variable_name  \\\n",
       "0  4129353.0          Edge  freight_parcel       22679.618    Edge_29460   \n",
       "1  4129353.0          Edge    freight_bulk       43134.521    Edge_10965   \n",
       "2  3374629.0          Edge  freight_parcel       58967.008    Edge_17348   \n",
       "3  3374629.0          Edge    freight_bulk      138302.430    Edge_20439   \n",
       "4  3078366.0          Edge    freight_bulk       43134.521    Edge_10971   \n",
       "\n",
       "   nx_edge_id  mode_oid  converted_capacity  converted_volume  \n",
       "0     14726.0  269353.0       540000.000000          118584.0  \n",
       "1      5479.0  269353.0       540000.000000          118584.0  \n",
       "2      8670.0  226482.0       563608.806599          207876.0  \n",
       "3     10216.0  226482.0       563608.806599          207876.0  \n",
       "4      5482.0   96162.0       328405.345291          127512.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ranked lists of edges to remove\n",
    "# (1) Keep only edges in the optimal solution\n",
    "# (2) Sort by sum_BC or volume\n",
    "# (3) Keep the columns we need\n",
    "# (4) Reset the index to assign rank\n",
    "\n",
    "# Note: in resiliency_disruptions.disrupt_network, the edges_remove DataFrame is sorted again by 'V' or 'BC'\n",
    "\n",
    "use_cols = ['edge_id', 'from_node_id', 'to_node_id', 'length', 'capacity', 'volume', 'sum_BC',\n",
    "            'variable_type', 'commodity_name', 'variable_value', 'variable_name', 'nx_edge_id', 'mode_oid', 'converted_capacity',\n",
    "            'converted_volume']\n",
    "\n",
    "if disrupt_type == 'V':\n",
    "    edges_remove = merge_opt[merge_opt['variable_value'] > 0].sort_values(by = 'volume', ascending = False).filter(items = use_cols).reset_index()\n",
    "elif disrupt_type == 'BC':\n",
    "    edges_remove = merge_opt[merge_opt['variable_value'] > 0].sort_values(by = 'sum_BC', ascending = False).filter(items = use_cols).reset_index()\n",
    "\n",
    "edges_remove.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list of edges to remove \n",
    "disrupt_root = os.path.join(os.path.split(scen_path)[0],\n",
    "                            '_'.join([os.path.split(scen_path)[1], disrupt_type, 'disrupt']))\n",
    "\n",
    "if not os.path.exists(disrupt_root):\n",
    "    os.mkdir(disrupt_root)\n",
    "\n",
    "edges_remove.to_csv(os.path.join(disrupt_root, 'Edges_to_Remove.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Scenarios, Disrupt Edges, and Run FTOT\n",
    "\n",
    "Create disrupted network by copying everything in `scen_path` to a new directory. Then overwrite the `networkx_edges` tables in that main.db with the disrupted versions.\n",
    "\n",
    "##### Assumptions:\n",
    "\n",
    "  1. ArcGIS Pro is installed.\n",
    "  2. The FTOT version being used has been modified according to the `README` in this directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 25 scenarios based on rs7_capacity\n",
      "Disrupted 25 scenarios\n"
     ]
    }
   ],
   "source": [
    "# Make new scenarios\n",
    "resiliency_disruptions.make_disruption_scenarios(disrupt_type, disrupt_steps, scen_path)\n",
    "\n",
    "# Apply disruption\n",
    "resiliency_disruptions.disrupt_network(disrupt_type, disrupt_steps, scen_path, edges_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running o1 for disrupt01\n",
      "Running o2 for disrupt01\n",
      "Running p for disrupt01\n",
      "Running d for disrupt01\n",
      "Preparing to search over o2_log_2024_04_17_10-40-58.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           01            0          0   497      2,412\n",
      "Running o1 for disrupt02\n",
      "Running o2 for disrupt02\n",
      "Running p for disrupt02\n",
      "Running d for disrupt02\n",
      "Preparing to search over o2_log_2024_04_17_10-43-32.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           02            0          0   494      2,413\n",
      "Running o1 for disrupt03\n",
      "Running o2 for disrupt03\n",
      "Running p for disrupt03\n",
      "Running d for disrupt03\n",
      "Preparing to search over o2_log_2024_04_17_10-46-55.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           03            0          0   554      2,421\n",
      "Running o1 for disrupt04\n",
      "Running o2 for disrupt04\n",
      "Running p for disrupt04\n",
      "Running d for disrupt04\n",
      "Preparing to search over o2_log_2024_04_17_10-49-45.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           04            0          0   607      2,583\n",
      "Running o1 for disrupt05\n",
      "Running o2 for disrupt05\n",
      "Running p for disrupt05\n",
      "Running d for disrupt05\n",
      "Preparing to search over o2_log_2024_04_17_10-52-38.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           05            0          0   607      2,583\n",
      "Running o1 for disrupt06\n",
      "Running o2 for disrupt06\n",
      "Running p for disrupt06\n",
      "Running d for disrupt06\n",
      "Preparing to search over o2_log_2024_04_17_10-56-07.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           06            0          0   607      2,583\n",
      "Running o1 for disrupt07\n",
      "Running o2 for disrupt07\n",
      "Running p for disrupt07\n",
      "Running d for disrupt07\n",
      "Preparing to search over o2_log_2024_04_17_10-59-22.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           07            0          0   607      2,583\n",
      "Running o1 for disrupt08\n",
      "Running o2 for disrupt08\n",
      "Running p for disrupt08\n",
      "Running d for disrupt08\n",
      "Preparing to search over o2_log_2024_04_17_11-02-38.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           08            0          0   607      2,583\n",
      "Running o1 for disrupt09\n",
      "Running o2 for disrupt09\n",
      "Running p for disrupt09\n",
      "Running d for disrupt09\n",
      "Preparing to search over o2_log_2024_04_17_11-05-52.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           09            0          0   607      2,583\n",
      "Running o1 for disrupt10\n",
      "Running o2 for disrupt10\n",
      "Running p for disrupt10\n",
      "Running d for disrupt10\n",
      "Preparing to search over o2_log_2024_04_17_11-09-00.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           10            0          0   607      2,583\n",
      "Running o1 for disrupt11\n",
      "Running o2 for disrupt11\n",
      "Running p for disrupt11\n",
      "Running d for disrupt11\n",
      "Preparing to search over o2_log_2024_04_17_11-11-55.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           11            0          0   607      2,583\n",
      "Running o1 for disrupt12\n",
      "Running o2 for disrupt12\n",
      "Running p for disrupt12\n",
      "Running d for disrupt12\n",
      "Preparing to search over o2_log_2024_04_17_11-14-58.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           12            0          0   607      2,583\n",
      "Running o1 for disrupt13\n",
      "Running o2 for disrupt13\n",
      "Running p for disrupt13\n",
      "Running d for disrupt13\n",
      "Preparing to search over o2_log_2024_04_17_11-18-23.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           13            0          0   607      2,583\n",
      "Running o1 for disrupt14\n",
      "Running o2 for disrupt14\n",
      "Running p for disrupt14\n",
      "Running d for disrupt14\n",
      "Preparing to search over o2_log_2024_04_17_11-21-22.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           14            0          0   607      2,583\n",
      "Running o1 for disrupt15\n",
      "Running o2 for disrupt15\n",
      "Running p for disrupt15\n",
      "Running d for disrupt15\n",
      "Preparing to search over o2_log_2024_04_17_11-24-21.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           15            0          0   607      2,583\n",
      "Running o1 for disrupt16\n",
      "Running o2 for disrupt16\n",
      "Running p for disrupt16\n",
      "Running d for disrupt16\n",
      "Preparing to search over o2_log_2024_04_17_11-27-21.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           16            0          0   607      2,583\n",
      "Running o1 for disrupt17\n",
      "Running o2 for disrupt17\n",
      "Running p for disrupt17\n",
      "Running d for disrupt17\n",
      "Preparing to search over o2_log_2024_04_17_11-30-20.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           17            0          0   607      2,583\n",
      "Running o1 for disrupt18\n",
      "Running o2 for disrupt18\n",
      "Running p for disrupt18\n",
      "Running d for disrupt18\n",
      "Preparing to search over o2_log_2024_04_17_11-33-15.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           18            0          0   607      2,583\n",
      "Running o1 for disrupt19\n",
      "Running o2 for disrupt19\n",
      "Running p for disrupt19\n",
      "Running d for disrupt19\n",
      "Preparing to search over o2_log_2024_04_17_11-35-49.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           19            0          0   607      2,583\n",
      "Running o1 for disrupt20\n",
      "Running o2 for disrupt20\n",
      "Running p for disrupt20\n",
      "Running d for disrupt20\n",
      "Preparing to search over o2_log_2024_04_17_11-39-03.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           20            0          0   607      2,583\n",
      "Running o1 for disrupt21\n",
      "Running o2 for disrupt21\n",
      "Running p for disrupt21\n",
      "Running d for disrupt21\n",
      "Preparing to search over o2_log_2024_04_17_11-41-36.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           21            0          0   607      2,583\n",
      "Running o1 for disrupt22\n",
      "Running o2 for disrupt22\n",
      "Running p for disrupt22\n",
      "Running d for disrupt22\n",
      "Preparing to search over o2_log_2024_04_17_11-44-23.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           22            0          0   607      2,583\n",
      "Running o1 for disrupt23\n",
      "Running o2 for disrupt23\n",
      "Running p for disrupt23\n",
      "Running d for disrupt23\n",
      "Preparing to search over o2_log_2024_04_17_11-47-22.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           23            0          0   607      2,583\n",
      "Running o1 for disrupt24\n",
      "Running o2 for disrupt24\n",
      "Running p for disrupt24\n",
      "Running d for disrupt24\n",
      "Preparing to search over o2_log_2024_04_17_11-49-52.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           24            0          0   607      2,583\n",
      "Running o1 for disrupt25\n",
      "Running o2 for disrupt25\n",
      "Running p for disrupt25\n",
      "Running d for disrupt25\n",
      "Preparing to search over o2_log_2024_04_17_11-52-49.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           25            0          0   607      2,583\n"
     ]
    }
   ],
   "source": [
    "# Begin running O through M steps of FTOT on the disupted scenarios\n",
    "# This may take several hours, depending on size of the network and number of disruption scenarios\n",
    "results = resiliency_disruptions.run_o_steps(disrupt_type, disrupt_steps, scen_path, PYTHON, FTOT, MAKE_MAPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Disruption Result Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "R_Process = subprocess.Popen(['Rscript.exe', 'compile_report.R', scen_path, disrupt_type],\n",
    "                 stdout = subprocess.PIPE, stderr = subprocess.PIPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\FTOT\\\\scenarios\\\\reference_scenarios\\\\rs7_capacity_BC_disrupt\\\\Disruption_Results_BC_25.html'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# move rendered HTML file when complete to the top-level disruption folder\n",
    "# this will replace any existing file\n",
    "here = os.getcwd()\n",
    "if not os.path.exists(os.path.join(here, 'Disruption_Results.html')):\n",
    "    print(\"OUTPUT FILE ERROR: Disruption_Results.html could not be found\")\n",
    "    raise Exception(\"OUTPUT FILE ERROR: Disruption_Results.html could not be found\")\n",
    "\n",
    "shutil.move(os.path.join(here, 'Disruption_Results.html'), os.path.join(disrupt_root, 'Disruption_Results_' +\n",
    "                                                                        disrupt_type + '_' + str(disrupt_steps) +\n",
    "                                                                        '.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webbrowser.open('file://' + os.path.realpath(os.path.join(disrupt_root, 'Disruption_Results_' + disrupt_type +\n",
    "                                                          '_' + str(disrupt_steps) + '.html')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
