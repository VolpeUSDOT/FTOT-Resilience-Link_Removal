{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential removal of links and resiliency testing\n",
    "\n",
    "### Overview\n",
    "The notebook iteratively removes the links (edges) in the FTOT road network in order of importance to create distinct disruption scenarios and re-runs FTOT to determine the new optimal solutions and costs. Importance is measured by:\n",
    "\n",
    "- the sum of __betweenness centrality__ of a link's beginning and ending points __OR__   \n",
    "- the __volume__ of background freight flows on a link.\n",
    "\n",
    "The notebook outputs (i) CSV files with information on the links removed and scenario results and (ii) interactive report generated with RMarkdown. Note that scenario costs in the outputs are the minimized FTOT objective value, which is based on impeded transport cost, facility build costs, and unmet demand penalties.\n",
    "\n",
    "### Instructions\n",
    "_Before running this notebook,_ follow instructions the repository's README to (i) set up and activate the Python environment and (ii) run a baseline FTOT scenario.\n",
    "\n",
    "__(1) Update parameters__ in the cell labeled Step 1:  \n",
    "- Baseline scenario name and path  \n",
    "- Measure of importance (volume or betweeness centrality)  \n",
    "- Number of disruption steps  \n",
    "- True/False toggle to export maps for each disruption scenario  \n",
    "     \n",
    "__(2) Run all cells__ by going to the top menu bar > Cell > Run All.\n",
    "\n",
    "__(3) Review outputs__ in the folder with disruption scenarios:\n",
    "\n",
    "- Edges_to_Remove.csv - a list of edges to remove with their importance ranking\n",
    "- Results.csv - resulting scenario costs and other optimal variables after each disruption step\n",
    "- Disruption_Results.html - interactive summary report\n",
    "\n",
    "The disruption folder will be created in the same location as the baseline scenario folder. The new folder will have `V_disrupt` or `BC_disrupt` appended to the scenario name.\n",
    "\n",
    "This notebook may take several hours to run depending on the scenario size and number of disruption steps.\n",
    "\n",
    "### Assumptions\n",
    "\n",
    "- You are working in a Python 3.x environment for this notebook. Refer to the README in this repository for setup instructions.\n",
    "- You have access to a ArcGIS license server if necessary\n",
    "- A baseline FTOT scenario was run with Network Density Reduction (NDR_On set to False in the scenario XML file) and with the swapped ftot_routing.py file from this repository."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import sqlalchemy\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "import momepy # for conversion from geopandas GeoDataFrame to networkX Graph\n",
    "import subprocess\n",
    "import shutil\n",
    "import webbrowser\n",
    "import resiliency_disruptions\n",
    "from osgeo import ogr\n",
    "import time\n",
    "\n",
    "PYTHON = r\"C:\\FTOT\\python3_env\\python.exe\"\n",
    "FTOT = r\"C:\\FTOT\\program\\ftot.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set User-Defined Parameters (USER INPUT REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Reference Scenario 7 as an example.\n",
    "# Modify `scen_name` and `scen_path` for your scenario.\n",
    "scen_name = 'rs7_capacity'\n",
    "scen_path = r'C:\\FTOT\\scenarios\\reference_scenarios\\rs7_capacity'\n",
    "\n",
    "# Enter disrupt_type 'BC' for betweeness centrality or 'V' for volume \n",
    "# Note: if background flows were not enabled in the baseline scenario,\n",
    "# the notebook will automatically switch to BC.\n",
    "disrupt_type = 'V'\n",
    "\n",
    "# Enter the number of disruption scenarios to generated\n",
    "# Recommend at least 25\n",
    "disrupt_steps = 25 \n",
    "\n",
    "# Set the variable `MAKE_MAPS` to `True` to output maps for each disruption scenario\n",
    "# Note this will increase runtime\n",
    "MAKE_MAPS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Importance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disrupt_type == 'BC':\n",
    "    \n",
    "    # Read in prepared betweeness centrality and road network graph data\n",
    "    # If these don't exist, the following steps will create them\n",
    "    picklename = os.path.join(scen_path, 'BetweenessG.pickle')\n",
    "    if os.path.exists(picklename):\n",
    "        file = open(picklename, 'rb')\n",
    "        betweenness_dict_road = pickle.load(file)\n",
    "        G_road = pickle.load(file)\n",
    "    \n",
    "    # Run betweenness centrality on the NetworkX graph\n",
    "    # Note: This step might take several minutes to a few hours   \n",
    "    elif not os.path.exists(picklename):\n",
    "        G_road = resiliency_disruptions.read_gdb(os.path.join(scen_path, 'main.gdb'), 'road')\n",
    "        print('Running Betweenness Centrality calculations. This might take more than 20 minutes.')\n",
    "        betweenness_dict_road = nx.betweenness_centrality(G_road, normalized=False, weight='Length')\n",
    "        print('Completed Betweenness Centrality calculations.')\n",
    "        \n",
    "        # Save with pickle\n",
    "        # Upon load, this pickle will contain the network G_road and the betweenness centrality dict and \n",
    "        with open(picklename, 'wb') as handle:\n",
    "            pickle.dump(betweenness_dict_road, handle)\n",
    "            pickle.dump(G_road, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Associate Importance Metrics with Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading in C:\\FTOT\\scenarios\\reference_scenarios\\rs7_capacity\n"
     ]
    }
   ],
   "source": [
    "# Read in FTOT data\n",
    "print('Reading in {}'.format(scen_path))\n",
    "\n",
    "db_name = 'main.db'\n",
    "db_path = 'sqlite:///' + os.path.join(scen_path, db_name)\n",
    "engine = sqlalchemy.create_engine(db_path)\n",
    "\n",
    "table_name = 'networkx_edges'\n",
    "nx_edges = pd.read_sql_table(table_name, engine)\n",
    "\n",
    "table_name = 'networkx_nodes'\n",
    "nx_nodes = pd.read_sql_table(table_name, engine)\n",
    "\n",
    "table_name = 'optimal_variables'\n",
    "optimal_vars = pd.read_sql_table(table_name, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Background flows confirmed\n"
     ]
    }
   ],
   "source": [
    "# Check whether scenario has background flow data\n",
    "# volume column in DB is filled with NULL if no\n",
    "# Automatically revert to betweeness centrality if no background flow data\n",
    "# TODO: Confirm want to switch if ANY are null?\n",
    "BACKGROUND_FLOWS = pd.isna(nx_edges['volume']).any()\n",
    "if BACKGROUND_FLOWS:\n",
    "    print('Background flows confirmed')\n",
    "elif disrupt_type == 'V' and not BACKGROUND_FLOWS:\n",
    "    print('WARNING: Network does not have background flows.')\n",
    "    print('Switching importance measure to betweenness centrality.')\n",
    "    disrupt_type = 'BC'\n",
    "else:\n",
    "    print('Scenario does not have background flows. Proceeding with disrupt type BC.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disrupt_type == 'BC':\n",
    "    \n",
    "    # Get shape_x and shape_y\n",
    "    road_orig_label_nodes = list(G_road.nodes)\n",
    "    node_shape_df_road = pd.DataFrame(road_orig_label_nodes)\n",
    "    \n",
    "    # Make the betweenness_centrality values as the framework to join in shape_x, shape_y, and node_id\n",
    "    bc_df_road = pd.DataFrame.from_dict(betweenness_dict_road, orient = 'index')\n",
    "    bc_df_road = bc_df_road.rename(columns = {0: 'BC'}).reset_index()\n",
    "    \n",
    "    bc_shape_df_road = pd.concat([bc_df_road, node_shape_df_road], axis = 1)\n",
    "    bc_shape_df_road = bc_shape_df_road.rename(columns = {0: 'shape_x', 1: 'shape_y'})\n",
    "        \n",
    "    # Now add node_id from networkx_nodes, using pandas merge with left join\n",
    "    # Use both shape_x and shape_y to identify the nodes correctly\n",
    "    bc_node_df = pd.merge(bc_shape_df_road, nx_nodes, on = ['shape_x', 'shape_y'], how = 'left')\n",
    "\n",
    "    # Now use this dataframe to populate a dataframe of edges\n",
    "    # We will want the following from networkx_edges:\n",
    "    # edge_id, from_node_id, to_node_id, mode_source, miles, mode_source_oid\n",
    "    # Then using the node_id column in the new bc_node_df, add these:\n",
    "    # from_node_BC, to_node_BC\n",
    "    # and sum those for sum_node_BC\n",
    "    merge_from = pd.merge(nx_edges, bc_node_df[['BC', 'node_id']],\n",
    "                          left_on = 'from_node_id',\n",
    "                          right_on = 'node_id',\n",
    "                          how = 'left')\n",
    "    merge_from = merge_from.rename(columns = {'BC': 'from_node_BC'})\n",
    "\n",
    "    merge_to = pd.merge(merge_from, bc_node_df[['BC', 'node_id']],\n",
    "                        left_on = 'to_node_id',\n",
    "                        right_on = 'node_id',\n",
    "                        how = 'left')\n",
    "    merge_to = merge_to.rename(columns = {'BC': 'to_node_BC'})\n",
    "\n",
    "    # Sum the BC values\n",
    "    merge_to['sum_BC'] = merge_to.filter(like = \"node_BC\").sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disrupt_type == 'V':\n",
    "    merge_to = nx_edges.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>edge_id</th>\n",
       "      <th>from_node_id</th>\n",
       "      <th>to_node_id</th>\n",
       "      <th>artificial</th>\n",
       "      <th>mode_source</th>\n",
       "      <th>mode_source_oid</th>\n",
       "      <th>length</th>\n",
       "      <th>route_cost_scaling</th>\n",
       "      <th>capacity</th>\n",
       "      <th>volume</th>\n",
       "      <th>...</th>\n",
       "      <th>limited_access</th>\n",
       "      <th>variable_type</th>\n",
       "      <th>var_id</th>\n",
       "      <th>variable_value</th>\n",
       "      <th>variable_name</th>\n",
       "      <th>nx_edge_id</th>\n",
       "      <th>mode_oid</th>\n",
       "      <th>converted_capacity</th>\n",
       "      <th>converted_volume</th>\n",
       "      <th>commodity_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>332</td>\n",
       "      <td>1908</td>\n",
       "      <td>2</td>\n",
       "      <td>road</td>\n",
       "      <td>274</td>\n",
       "      <td>3.541860</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>399</td>\n",
       "      <td>1909</td>\n",
       "      <td>2</td>\n",
       "      <td>road</td>\n",
       "      <td>341</td>\n",
       "      <td>0.275609</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>438</td>\n",
       "      <td>1910</td>\n",
       "      <td>2</td>\n",
       "      <td>road</td>\n",
       "      <td>380</td>\n",
       "      <td>0.087752</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>448</td>\n",
       "      <td>1911</td>\n",
       "      <td>2</td>\n",
       "      <td>road</td>\n",
       "      <td>390</td>\n",
       "      <td>0.129394</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>452</td>\n",
       "      <td>1912</td>\n",
       "      <td>2</td>\n",
       "      <td>road</td>\n",
       "      <td>394</td>\n",
       "      <td>0.238547</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>-9999.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   edge_id  from_node_id  to_node_id  artificial mode_source  mode_source_oid  \\\n",
       "0        1           332        1908           2        road              274   \n",
       "1        2           399        1909           2        road              341   \n",
       "2        3           438        1910           2        road              380   \n",
       "3        4           448        1911           2        road              390   \n",
       "4        5           452        1912           2        road              394   \n",
       "\n",
       "     length  route_cost_scaling  capacity  volume  ...  limited_access  \\\n",
       "0  3.541860                 1.0       NaN     NaN  ...         -9999.0   \n",
       "1  0.275609                 1.0       NaN     NaN  ...         -9999.0   \n",
       "2  0.087752                 1.0       NaN     NaN  ...         -9999.0   \n",
       "3  0.129394                 1.0       NaN     NaN  ...         -9999.0   \n",
       "4  0.238547                 1.0       NaN     NaN  ...         -9999.0   \n",
       "\n",
       "   variable_type  var_id variable_value  variable_name  nx_edge_id mode_oid  \\\n",
       "0            NaN     NaN            NaN            NaN         NaN      NaN   \n",
       "1            NaN     NaN            NaN            NaN         NaN      NaN   \n",
       "2            NaN     NaN            NaN            NaN         NaN      NaN   \n",
       "3            NaN     NaN            NaN            NaN         NaN      NaN   \n",
       "4            NaN     NaN            NaN            NaN         NaN      NaN   \n",
       "\n",
       "   converted_capacity  converted_volume  commodity_name  \n",
       "0                 NaN               NaN             NaN  \n",
       "1                 NaN               NaN             NaN  \n",
       "2                 NaN               NaN             NaN  \n",
       "3                 NaN               NaN             NaN  \n",
       "4                 NaN               NaN             NaN  \n",
       "\n",
       "[5 rows x 22 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select optimal_vars DB columns to keep \n",
    "use_opt_vars = ['variable_type',\n",
    "               'var_id',\n",
    "               'variable_value',\n",
    "                'variable_name',\n",
    "                'nx_edge_id',\n",
    "                'mode_oid',\n",
    "                'converted_capacity',\n",
    "                'converted_volume',\n",
    "                'commodity_name'\n",
    "               ]\n",
    "\n",
    "merge_opt = pd.merge(merge_to, optimal_vars[use_opt_vars],\n",
    "                     left_on = 'edge_id',\n",
    "                     right_on = 'nx_edge_id',\n",
    "                     how = 'left')\n",
    "\n",
    "merge_opt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>edge_id</th>\n",
       "      <th>from_node_id</th>\n",
       "      <th>to_node_id</th>\n",
       "      <th>length</th>\n",
       "      <th>capacity</th>\n",
       "      <th>volume</th>\n",
       "      <th>variable_type</th>\n",
       "      <th>commodity_name</th>\n",
       "      <th>variable_value</th>\n",
       "      <th>variable_name</th>\n",
       "      <th>nx_edge_id</th>\n",
       "      <th>mode_oid</th>\n",
       "      <th>converted_capacity</th>\n",
       "      <th>converted_volume</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5482</td>\n",
       "      <td>5483</td>\n",
       "      <td>4162</td>\n",
       "      <td>7208</td>\n",
       "      <td>0.071771</td>\n",
       "      <td>69446.587921</td>\n",
       "      <td>47941.5</td>\n",
       "      <td>Edge</td>\n",
       "      <td>freight_parcel</td>\n",
       "      <td>22679.618</td>\n",
       "      <td>Edge_10974</td>\n",
       "      <td>5483.0</td>\n",
       "      <td>249139.0</td>\n",
       "      <td>1.666718e+06</td>\n",
       "      <td>1150596.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3029</td>\n",
       "      <td>3030</td>\n",
       "      <td>3115</td>\n",
       "      <td>3664</td>\n",
       "      <td>0.318439</td>\n",
       "      <td>45000.000000</td>\n",
       "      <td>41687.0</td>\n",
       "      <td>Edge</td>\n",
       "      <td>freight_bulk</td>\n",
       "      <td>75443.128</td>\n",
       "      <td>Edge_6067</td>\n",
       "      <td>3030.0</td>\n",
       "      <td>80118.0</td>\n",
       "      <td>1.080000e+06</td>\n",
       "      <td>1000488.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12538</td>\n",
       "      <td>12539</td>\n",
       "      <td>7188</td>\n",
       "      <td>1919</td>\n",
       "      <td>0.031103</td>\n",
       "      <td>47745.379525</td>\n",
       "      <td>40784.0</td>\n",
       "      <td>Edge</td>\n",
       "      <td>freight_bulk</td>\n",
       "      <td>43134.521</td>\n",
       "      <td>Edge_25085</td>\n",
       "      <td>12539.0</td>\n",
       "      <td>249115.0</td>\n",
       "      <td>1.145889e+06</td>\n",
       "      <td>978816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>60</td>\n",
       "      <td>61</td>\n",
       "      <td>1919</td>\n",
       "      <td>7215</td>\n",
       "      <td>0.828394</td>\n",
       "      <td>47745.379525</td>\n",
       "      <td>40784.0</td>\n",
       "      <td>Edge</td>\n",
       "      <td>freight_bulk</td>\n",
       "      <td>43134.521</td>\n",
       "      <td>Edge_129</td>\n",
       "      <td>61.0</td>\n",
       "      <td>249561.0</td>\n",
       "      <td>1.145889e+06</td>\n",
       "      <td>978816.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>14861</td>\n",
       "      <td>14862</td>\n",
       "      <td>8239</td>\n",
       "      <td>8236</td>\n",
       "      <td>1.454485</td>\n",
       "      <td>47739.607172</td>\n",
       "      <td>38842.5</td>\n",
       "      <td>Edge</td>\n",
       "      <td>freight_bulk</td>\n",
       "      <td>43134.521</td>\n",
       "      <td>Edge_29731</td>\n",
       "      <td>14862.0</td>\n",
       "      <td>270884.0</td>\n",
       "      <td>1.145751e+06</td>\n",
       "      <td>932220.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index  edge_id  from_node_id  to_node_id    length      capacity   volume  \\\n",
       "0   5482     5483          4162        7208  0.071771  69446.587921  47941.5   \n",
       "1   3029     3030          3115        3664  0.318439  45000.000000  41687.0   \n",
       "2  12538    12539          7188        1919  0.031103  47745.379525  40784.0   \n",
       "3     60       61          1919        7215  0.828394  47745.379525  40784.0   \n",
       "4  14861    14862          8239        8236  1.454485  47739.607172  38842.5   \n",
       "\n",
       "  variable_type  commodity_name  variable_value variable_name  nx_edge_id  \\\n",
       "0          Edge  freight_parcel       22679.618    Edge_10974      5483.0   \n",
       "1          Edge    freight_bulk       75443.128     Edge_6067      3030.0   \n",
       "2          Edge    freight_bulk       43134.521    Edge_25085     12539.0   \n",
       "3          Edge    freight_bulk       43134.521      Edge_129        61.0   \n",
       "4          Edge    freight_bulk       43134.521    Edge_29731     14862.0   \n",
       "\n",
       "   mode_oid  converted_capacity  converted_volume  \n",
       "0  249139.0        1.666718e+06         1150596.0  \n",
       "1   80118.0        1.080000e+06         1000488.0  \n",
       "2  249115.0        1.145889e+06          978816.0  \n",
       "3  249561.0        1.145889e+06          978816.0  \n",
       "4  270884.0        1.145751e+06          932220.0  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create ranked lists of edges to remove\n",
    "# (1) Keep only edges in the optimal solution\n",
    "# (2) Sort by sum_BC or volume\n",
    "# (3) Keep the columns we need\n",
    "# (4) Reset the index to assign rank\n",
    "\n",
    "# Note: in resiliency_disruptions.disrupt_network, the edges_remove DataFrame is sorted again by 'V' or 'BC'\n",
    "\n",
    "use_cols = ['edge_id', 'from_node_id', 'to_node_id', 'length', 'capacity', 'volume', 'sum_BC',\n",
    "            'variable_type', 'commodity_name', 'variable_value', 'variable_name', 'nx_edge_id', 'mode_oid', 'converted_capacity',\n",
    "            'converted_volume']\n",
    "\n",
    "if disrupt_type == 'V':\n",
    "    edges_remove = merge_opt[merge_opt['variable_value'] > 0].sort_values(by = 'volume', ascending = False).filter(items = use_cols).reset_index()\n",
    "elif disrupt_type == 'BC':\n",
    "    edges_remove = merge_opt[merge_opt['variable_value'] > 0].sort_values(by = 'sum_BC', ascending = False).filter(items = use_cols).reset_index()\n",
    "\n",
    "edges_remove.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list of edges to remove \n",
    "disrupt_root = os.path.join(os.path.split(scen_path)[0],\n",
    "                            '_'.join([os.path.split(scen_path)[1], disrupt_type, 'disrupt']))\n",
    "\n",
    "if not os.path.exists(disrupt_root):\n",
    "    os.mkdir(disrupt_root)\n",
    "\n",
    "edges_remove.to_csv(os.path.join(disrupt_root, 'Edges_to_Remove.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Scenarios, Disrupt Edges, and Run FTOT\n",
    "\n",
    "Create disrupted network by copying everything in `scen_path` to a new directory. Then overwrite the `networkx_edges` tables in that main.db with the disrupted versions.\n",
    "\n",
    "##### Assumptions:\n",
    "\n",
    "  1. ArcGIS Pro is installed.\n",
    "  2. The FTOT version being used has been modified according to the `README` in this directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prepared 25 scenarios based on rs7_capacity\n",
      "Disrupted 25 scenarios\n"
     ]
    }
   ],
   "source": [
    "# Make new scenarios\n",
    "resiliency_disruptions.make_disruption_scenarios(disrupt_type, disrupt_steps, scen_path)\n",
    "\n",
    "# Apply disruption\n",
    "resiliency_disruptions.disrupt_network(disrupt_type, disrupt_steps, scen_path, edges_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running o1 for disrupt01\n",
      "Running o2 for disrupt01\n",
      "Running p for disrupt01\n",
      "Running d for disrupt01\n",
      "Preparing to search over o2_log_2024_04_18_09-05-50.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           01            0          0   497      2,527\n",
      "Running o1 for disrupt02\n",
      "Running o2 for disrupt02\n",
      "Running p for disrupt02\n",
      "Running d for disrupt02\n",
      "Preparing to search over o2_log_2024_04_18_09-08-30.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           02            0          0   493      2,529\n",
      "Running o1 for disrupt03\n",
      "Running o2 for disrupt03\n",
      "Running p for disrupt03\n",
      "Running d for disrupt03\n",
      "Preparing to search over o2_log_2024_04_18_09-11-28.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           03            0          0   488      2,529\n",
      "Running o1 for disrupt04\n",
      "Running o2 for disrupt04\n",
      "Running p for disrupt04\n",
      "Running d for disrupt04\n",
      "Preparing to search over o2_log_2024_04_18_09-14-45.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           04            0          0   488      2,529\n",
      "Running o1 for disrupt05\n",
      "Running o2 for disrupt05\n",
      "Running p for disrupt05\n",
      "Running d for disrupt05\n",
      "Preparing to search over o2_log_2024_04_18_09-17-42.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           05            0          0   494      2,543\n",
      "Running o1 for disrupt06\n",
      "Running o2 for disrupt06\n",
      "Running p for disrupt06\n",
      "Running d for disrupt06\n",
      "Preparing to search over o2_log_2024_04_18_09-20-29.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           06            0          0   494      2,543\n",
      "Running o1 for disrupt07\n",
      "Running o2 for disrupt07\n",
      "Running p for disrupt07\n",
      "Running d for disrupt07\n",
      "Preparing to search over o2_log_2024_04_18_09-23-18.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           07            0          0   494      2,543\n",
      "Running o1 for disrupt08\n",
      "Running o2 for disrupt08\n",
      "Running p for disrupt08\n",
      "Running d for disrupt08\n",
      "Preparing to search over o2_log_2024_04_18_09-26-28.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           08            0          0   495      2,543\n",
      "Running o1 for disrupt09\n",
      "Running o2 for disrupt09\n",
      "Running p for disrupt09\n",
      "Running d for disrupt09\n",
      "Preparing to search over o2_log_2024_04_18_09-29-22.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           09            0          0   495      2,543\n",
      "Running o1 for disrupt10\n",
      "Running o2 for disrupt10\n",
      "Running p for disrupt10\n",
      "Running d for disrupt10\n",
      "Preparing to search over o2_log_2024_04_18_09-32-06.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           10            0          0   495      2,543\n",
      "Running o1 for disrupt11\n",
      "Running o2 for disrupt11\n",
      "Running p for disrupt11\n",
      "Running d for disrupt11\n",
      "Preparing to search over o2_log_2024_04_18_09-34-45.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           11            0          0   489      2,543\n",
      "Running o1 for disrupt12\n",
      "Running o2 for disrupt12\n",
      "Running p for disrupt12\n",
      "Running d for disrupt12\n",
      "Preparing to search over o2_log_2024_04_18_09-38-36.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           12            0          0   489      2,543\n",
      "Running o1 for disrupt13\n",
      "Running o2 for disrupt13\n",
      "Running p for disrupt13\n",
      "Running d for disrupt13\n",
      "Preparing to search over o2_log_2024_04_18_09-42-28.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           13            0          0   489      2,543\n",
      "Running o1 for disrupt14\n",
      "Running o2 for disrupt14\n",
      "Running p for disrupt14\n",
      "Running d for disrupt14\n",
      "Preparing to search over o2_log_2024_04_18_09-46-22.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           14            0          0   514      2,602\n",
      "Running o1 for disrupt15\n",
      "Running o2 for disrupt15\n",
      "Running p for disrupt15\n",
      "Running d for disrupt15\n",
      "Preparing to search over o2_log_2024_04_18_09-49-58.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           15            0          0   514      2,602\n",
      "Running o1 for disrupt16\n",
      "Running o2 for disrupt16\n",
      "Running p for disrupt16\n",
      "Running d for disrupt16\n",
      "Preparing to search over o2_log_2024_04_18_09-53-59.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           16            0          0   514      2,602\n",
      "Running o1 for disrupt17\n",
      "Running o2 for disrupt17\n",
      "Running p for disrupt17\n",
      "Running d for disrupt17\n",
      "Preparing to search over o2_log_2024_04_18_09-57-57.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           17            0          0   514      2,602\n",
      "Running o1 for disrupt18\n",
      "Running o2 for disrupt18\n",
      "Running p for disrupt18\n",
      "Running d for disrupt18\n",
      "Preparing to search over o2_log_2024_04_18_10-01-53.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           18            0          0   514      2,602\n",
      "Running o1 for disrupt19\n",
      "Running o2 for disrupt19\n",
      "Running p for disrupt19\n",
      "Running d for disrupt19\n",
      "Preparing to search over o2_log_2024_04_18_10-05-46.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           19            0          0   514      2,603\n",
      "Running o1 for disrupt20\n",
      "Running o2 for disrupt20\n",
      "Running p for disrupt20\n",
      "Running d for disrupt20\n",
      "Preparing to search over o2_log_2024_04_18_10-09-49.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           20            0          0   520      2,608\n",
      "Running o1 for disrupt21\n",
      "Running o2 for disrupt21\n",
      "Running p for disrupt21\n",
      "Running d for disrupt21\n",
      "Preparing to search over o2_log_2024_04_18_10-13-40.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           21            0          0   520      2,608\n",
      "Running o1 for disrupt22\n",
      "Running o2 for disrupt22\n",
      "Running p for disrupt22\n",
      "Running d for disrupt22\n",
      "Preparing to search over o2_log_2024_04_18_10-17-59.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           22            0          0   535      2,610\n",
      "Running o1 for disrupt23\n",
      "Running o2 for disrupt23\n",
      "Running p for disrupt23\n",
      "Running d for disrupt23\n",
      "Preparing to search over o2_log_2024_04_18_10-22-00.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           23            0          0   535      2,610\n",
      "Running o1 for disrupt24\n",
      "Running o2 for disrupt24\n",
      "Running p for disrupt24\n",
      "Running d for disrupt24\n",
      "Preparing to search over o2_log_2024_04_18_10-25-59.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           24            0          0   547      2,673\n",
      "Running o1 for disrupt25\n",
      "Running o2 for disrupt25\n",
      "Running p for disrupt25\n",
      "Running d for disrupt25\n",
      "Preparing to search over o2_log_2024_04_18_10-29-54.log\n",
      "  disrupt_step unmet_demand unmet_cost nedge total_cost\n",
      "0           25            0          0   547      2,673\n"
     ]
    }
   ],
   "source": [
    "# Begin running O through M steps of FTOT on the disupted scenarios\n",
    "# This may take several hours, depending on size of the network and number of disruption scenarios\n",
    "results = resiliency_disruptions.run_o_steps(disrupt_type, disrupt_steps, scen_path, PYTHON, FTOT, MAKE_MAPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Disruption Result Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\FTOT\\\\scenarios\\\\reference_scenarios\\\\rs7_capacity_V_disrupt\\\\Disruption_Results_V_25.html'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R_Process = subprocess.Popen(['Rscript.exe', 'compile_report.R', scen_path, disrupt_type],\n",
    "                 stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "time.sleep(20) # Pause for 20 seconds\n",
    "\n",
    "# move rendered HTML file when complete to the top-level disruption folder\n",
    "# this will replace any existing file\n",
    "here = os.getcwd()\n",
    "if not os.path.exists(os.path.join(here, 'Disruption_Results.html')):\n",
    "    print(\"OUTPUT FILE ERROR: Disruption_Results.html could not be found\")\n",
    "    raise Exception(\"OUTPUT FILE ERROR: Disruption_Results.html could not be found\")\n",
    "\n",
    "\n",
    "disrupt_root = scen_path + \"_\" + disrupt_type + \"_disrupt\"\n",
    "shutil.move(os.path.join(here, 'Disruption_Results.html'), os.path.join(disrupt_root, 'Disruption_Results_' +\n",
    "                                                                        disrupt_type + '_' + str(disrupt_steps) +\n",
    "                                                                        '.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "webbrowser.open('file://' + os.path.realpath(os.path.join(disrupt_root, 'Disruption_Results_' + disrupt_type +\n",
    "                                                          '_' + str(disrupt_steps) + '.html')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
