{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential removal of links and resiliency testing\n",
    "\n",
    "### Overview\n",
    "The notebook iteratively removes links (edges) in the FTOT road network in order of importance to create distinct disruption scenarios and re-runs FTOT to determine the new optimal solutions and costs. Importance is measured by:\n",
    "\n",
    "- the sum of __betweenness centrality__ of a link's beginning and ending points __OR__   \n",
    "- the __volume__ of background freight flows on a link.\n",
    "\n",
    "The notebook outputs (i) CSV files with information on the links removed and corresponding scenario results and (ii) an interactive report generated with RMarkdown. Note that scenario costs in the outputs are the minimized FTOT objective value, which is based on impeded transport cost, facility build costs, and unmet demand penalties.\n",
    "\n",
    "### Instructions\n",
    "_Before running this notebook,_ follow instructions in the repository's README to (i) set up and activate the Python environment and (ii) run a baseline FTOT scenario.\n",
    "\n",
    "__(1) Update parameters__ in the cell labeled Step 1:\n",
    "- Baseline scenario name and path\n",
    "- Measure of importance (volume or betweenness centrality)\n",
    "- Number of disruption steps\n",
    "- True/False toggle to export maps for each disruption scenario\n",
    "\n",
    "__(2) Run all cells__ by going to the top menu bar > Cell > Run All.\n",
    "\n",
    "__(3) Review outputs__ in the folder with disruption scenarios:\n",
    "\n",
    "- Edges_to_Remove.csv - a list of edges to remove with their importance ranking\n",
    "- Results.csv - resulting scenario costs and other optimal solution metrics after each disruption step\n",
    "- Disruption_Results.html - interactive summary report\n",
    "\n",
    "The disruption folder will be created in the same location as the baseline scenario folder. The new folder will have `V_disrupt` or `BC_disrupt` appended to the scenario name.\n",
    "\n",
    "This notebook may take several hours to run depending on the scenario size and number of disruption steps.\n",
    "\n",
    "### Troubleshooting HTML Report Creation\n",
    "The final cells in the Conduct_Link_Removal.ipynb notebook generate the HTML report. If these cells error out and you do not see an HTML report pop up in your browser or appear in the disruption folder (located at the same place as your baseline scenario folder), please try the following steps:\n",
    "\n",
    "1) Open a new Anaconda Prompt window and activate the environment by running `conda activate FTOTnetworkEnv`\n",
    "2) Navigate to the link_removal subfolder: `cd C:\\github\\FTOT-Resilience-Link_Removal\\link_removal`\n",
    "3) Run the following: `Rscript compile_report.R <BASELINE SCENARIO FOLDER> <DISRUPT TYPE>`. For example, for Reference Scenario 7 and disruption type 'V' for volume, run `Rscript compile_report.R C:\\FTOT\\scenarios\\reference_scenarios\\rs7_capacity V`\n",
    "4) If successful, the prompt window should print out \"Output created: Disruption_Results.html\".\n",
    "5) Scroll back down to Step 5 and re-run each cell underneath \"Step 5\" one-by-one by clicking into each cell, then from the top menu bar running Cell > Run Cells. You can alternatively click into each cell and press Ctrl + Enter.\n",
    "6) The report should appear in your browser window and should be saved in the disruption folder.\n",
    "\n",
    "If needed, reach out to the FTOT team at FTOT-Team@dot.gov for assistance.\n",
    "\n",
    "### Assumptions\n",
    "- You are working in a Python 3.x environment for this notebook. Refer to the README in this repository for setup instructions.\n",
    "- You have access to an ArcGIS license server.\n",
    "- A baseline FTOT scenario was run with Network Density Reduction (NDR_On) set to False in the scenario XML file and with the modified `program` folder from this repository in place of the baseline FTOT `program` folder."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Load Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sqlalchemy\n",
    "import networkx as nx\n",
    "import os\n",
    "import pickle\n",
    "import subprocess\n",
    "import shutil\n",
    "import webbrowser\n",
    "import resiliency_disruptions\n",
    "from osgeo import ogr\n",
    "import time\n",
    "\n",
    "PYTHON = r\"C:\\FTOT\\python3_env\\python.exe\"\n",
    "FTOT = r\"C:\\FTOT\\program\\ftot.py\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Set User-Defined Parameters (USER INPUT REQUIRED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses Reference Scenario 7 as an example.\n",
    "# Modify `scen_name` and `scen_path` for your scenario.\n",
    "scen_name = 'rs7_capacity'\n",
    "scen_path = r'C:\\FTOT\\scenarios\\reference_scenarios\\rs7_capacity'\n",
    "\n",
    "# Enter disrupt_type 'BC' for betweenness centrality or 'V' for volume.\n",
    "# Note: If background flows were not enabled in the baseline scenario,\n",
    "# the notebook will automatically switch to BC.\n",
    "disrupt_type = 'V'\n",
    "\n",
    "# Enter the number of disruption scenarios to generated.\n",
    "# Recommend at least 25.\n",
    "disrupt_steps = 25\n",
    "\n",
    "# Set the variable `MAKE_MAPS` to `True` to you wish to output maps for each disruption scenario.\n",
    "# Note this will increase runtime.\n",
    "MAKE_MAPS = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Calculate Importance Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disrupt_type == 'BC':\n",
    "    \n",
    "    # Read in prepared betweenness centrality and road network graph data\n",
    "    # If these don't exist, the following steps will create them\n",
    "    picklename = os.path.join(scen_path, 'BetweenessG.pickle')\n",
    "    if os.path.exists(picklename):\n",
    "        file = open(picklename, 'rb')\n",
    "        betweenness_dict_road = pickle.load(file)\n",
    "        G_road = pickle.load(file)\n",
    "    \n",
    "    # Run betweenness centrality on the NetworkX graph\n",
    "    # Note: This step might take several minutes to a few hours\n",
    "    elif not os.path.exists(picklename):\n",
    "        G_road = resiliency_disruptions.read_gdb(os.path.join(scen_path, 'main.gdb'), 'road')\n",
    "        print('Running Betweenness Centrality calculations. This might take more than 20 minutes.')\n",
    "        betweenness_dict_road = nx.betweenness_centrality(G_road, normalized=False, weight='Length')\n",
    "        print('Completed Betweenness Centrality calculations.')\n",
    "        \n",
    "        # Save with pickle\n",
    "        # Upon load, this pickle will contain the network G_road and the betweenness centrality dict\n",
    "        with open(picklename, 'wb') as handle:\n",
    "            pickle.dump(betweenness_dict_road, handle)\n",
    "            pickle.dump(G_road, handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Associate Importance Metrics with Edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in FTOT data\n",
    "print('Reading in {}'.format(scen_path))\n",
    "\n",
    "db_name = 'main.db'\n",
    "db_path = 'sqlite:///' + os.path.join(scen_path, db_name)\n",
    "engine = sqlalchemy.create_engine(db_path)\n",
    "\n",
    "table_name = 'networkx_edges'\n",
    "nx_edges = pd.read_sql_table(table_name, engine)\n",
    "\n",
    "table_name = 'networkx_nodes'\n",
    "nx_nodes = pd.read_sql_table(table_name, engine)\n",
    "\n",
    "table_name = 'optimal_variables'\n",
    "optimal_vars = pd.read_sql_table(table_name, engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check whether scenario has background flow data\n",
    "# Volume column in DB is filled with NULL if not\n",
    "# Automatically revert to betweenness centrality if no background flow data\n",
    "BACKGROUND_FLOWS = pd.isna(nx_edges['volume']).any()\n",
    "if BACKGROUND_FLOWS:\n",
    "    print('Background flows confirmed')\n",
    "elif disrupt_type == 'V' and not BACKGROUND_FLOWS:\n",
    "    print('WARNING: Network does not have background flows.')\n",
    "    print('Switching importance measure to betweenness centrality.')\n",
    "    disrupt_type = 'BC'\n",
    "else:\n",
    "    print('Scenario does not have background flows. Proceeding with disrupt type BC.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disrupt_type == 'BC':\n",
    "    \n",
    "    # Get shape_x and shape_y\n",
    "    road_orig_label_nodes = list(G_road.nodes)\n",
    "    node_shape_df_road = pd.DataFrame(road_orig_label_nodes)\n",
    "    \n",
    "    # Make the betweenness_centrality values as the framework to join in shape_x, shape_y, and node_id\n",
    "    bc_df_road = pd.DataFrame.from_dict(betweenness_dict_road, orient = 'index')\n",
    "    bc_df_road = bc_df_road.rename(columns = {0: 'BC'}).reset_index()\n",
    "    \n",
    "    bc_shape_df_road = pd.concat([bc_df_road, node_shape_df_road], axis = 1)\n",
    "    bc_shape_df_road = bc_shape_df_road.rename(columns = {0: 'shape_x', 1: 'shape_y'})\n",
    "        \n",
    "    # Now add node_id from networkx_nodes, using pandas merge with left join\n",
    "    # Use both shape_x and shape_y to identify the nodes correctly\n",
    "    bc_node_df = pd.merge(bc_shape_df_road, nx_nodes, on = ['shape_x', 'shape_y'], how = 'left')\n",
    "\n",
    "    # Now use this dataframe to populate a dataframe of edges\n",
    "    # We will want the following from networkx_edges:\n",
    "    # edge_id, from_node_id, to_node_id, mode_source, miles, mode_source_oid\n",
    "    # Then using the node_id column in the new bc_node_df, add these:\n",
    "    # from_node_BC, to_node_BC\n",
    "    # and sum those for sum_node_BC\n",
    "    merge_from = pd.merge(nx_edges, bc_node_df[['BC', 'node_id']],\n",
    "                          left_on = 'from_node_id',\n",
    "                          right_on = 'node_id',\n",
    "                          how = 'left')\n",
    "    merge_from = merge_from.rename(columns = {'BC': 'from_node_BC'})\n",
    "\n",
    "    merge_to = pd.merge(merge_from, bc_node_df[['BC', 'node_id']],\n",
    "                        left_on = 'to_node_id',\n",
    "                        right_on = 'node_id',\n",
    "                        how = 'left')\n",
    "    merge_to = merge_to.rename(columns = {'BC': 'to_node_BC'})\n",
    "\n",
    "    # Sum the BC values\n",
    "    merge_to['sum_BC'] = merge_to.filter(like = \"node_BC\").sum(axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if disrupt_type == 'V':\n",
    "    merge_to = nx_edges.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select optimal_vars DB columns to keep\n",
    "use_opt_vars = ['variable_type',\n",
    "                'var_id',\n",
    "                'variable_value',\n",
    "                'variable_name',\n",
    "                'nx_edge_id',\n",
    "                'mode_oid',\n",
    "                'converted_capacity',\n",
    "                'converted_volume',\n",
    "                'commodity_name'\n",
    "               ]\n",
    "\n",
    "merge_opt = pd.merge(merge_to, optimal_vars[use_opt_vars],\n",
    "                     left_on = 'edge_id',\n",
    "                     right_on = 'nx_edge_id',\n",
    "                     how = 'left')\n",
    "\n",
    "merge_opt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ranked list of edges to remove\n",
    "# (1) Keep only edges in the optimal solution\n",
    "# (2) Sort by sum_BC or volume\n",
    "# (3) Keep the columns we need\n",
    "# (4) Reset the index to assign rank\n",
    "\n",
    "# Note: in resiliency_disruptions.disrupt_network, the edges_remove DataFrame is sorted again by 'V' or 'BC'\n",
    "\n",
    "use_cols = ['edge_id', 'from_node_id', 'to_node_id', 'length', 'capacity', 'volume', 'sum_BC',\n",
    "            'variable_type', 'commodity_name', 'variable_value', 'variable_name', 'nx_edge_id', 'mode_oid', 'converted_capacity',\n",
    "            'converted_volume']\n",
    "\n",
    "if disrupt_type == 'V':\n",
    "    edges_remove = merge_opt[merge_opt['variable_value'] > 0].sort_values(by = 'volume', ascending = False).filter(items = use_cols).reset_index()\n",
    "elif disrupt_type == 'BC':\n",
    "    edges_remove = merge_opt[merge_opt['variable_value'] > 0].sort_values(by = 'sum_BC', ascending = False).filter(items = use_cols).reset_index()\n",
    "\n",
    "edges_remove.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export list of edges to remove\n",
    "disrupt_root = os.path.join(os.path.split(scen_path)[0],\n",
    "                            '_'.join([os.path.split(scen_path)[1], disrupt_type, 'disrupt']))\n",
    "\n",
    "if not os.path.exists(disrupt_root):\n",
    "    os.mkdir(disrupt_root)\n",
    "\n",
    "edges_remove.to_csv(os.path.join(disrupt_root, 'Edges_to_Remove.csv'), index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Create Scenarios, Disrupt Edges, and Run FTOT\n",
    "\n",
    "Create disrupted network by copying everything in `scen_path` to a new directory and overwriting the `networkx_edge_costs` table in the main.db with the disrupted versions of network edge cost.\n",
    "\n",
    "##### Assumptions:\n",
    "\n",
    "  1. ArcGIS Pro is installed and the license is accessible.\n",
    "  2. The FTOT version being used has been modified according to the `README` in this directory.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new scenarios\n",
    "resiliency_disruptions.make_disruption_scenarios(disrupt_type, disrupt_steps, scen_path)\n",
    "\n",
    "# Apply disruptions\n",
    "resiliency_disruptions.disrupt_network(disrupt_type, disrupt_steps, scen_path, edges_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run O through D (and optionally M) steps of FTOT on the disupted scenarios\n",
    "# This may take several hours, depending on size of the network and number of disruption scenarios\n",
    "results = resiliency_disruptions.run_o_steps(disrupt_type, disrupt_steps, scen_path, PYTHON, FTOT, MAKE_MAPS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Generate Disruption Results Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "here = os.getcwd()\n",
    "\n",
    "# Delete existing HTML in repository if found\n",
    "if os.path.exists(os.path.join(here, 'Disruption_Results.html')):\n",
    "    os.remove(os.path.join(here, 'Disruption_Results.html'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Render RMarkdown report\n",
    "R_Process = subprocess.Popen(['Rscript.exe', 'compile_report.R', scen_path, disrupt_type],\n",
    "                             stdout = subprocess.PIPE, stderr = subprocess.PIPE)\n",
    "\n",
    "print ('Rendering report...')\n",
    "\n",
    "# Allow report time to render\n",
    "# Exit while loop if HTML report is found AND knit.md file is no longer found\n",
    "# Or time out after 60 seconds\n",
    "timer = 0\n",
    "while timer <= 60 and (not os.path.exists(os.path.join(here, 'Disruption_Results.html')) or os.path.exists(os.path.join(here, 'Disruption_Results.knit.md'))):\n",
    "    time.sleep(5) # Pause for 5 seconds\n",
    "    timer += 5\n",
    "\n",
    "# Move rendered HTML file when complete to the top-level disruption folder\n",
    "# This will replace any existing file\n",
    "if not os.path.exists(os.path.join(here, 'Disruption_Results.html')):\n",
    "    print(\"OUTPUT FILE ERROR: Disruption_Results.html could not be found\")\n",
    "    raise Exception(\"OUTPUT FILE ERROR: Disruption_Results.html could not be found\")\n",
    "\n",
    "disrupt_root = scen_path + \"_\" + disrupt_type + \"_disrupt\"\n",
    "new_html_location =  os.path.join(disrupt_root, 'Disruption_Results_' + disrupt_type + '_' + str(disrupt_steps) +'.html')\n",
    "shutil.move(os.path.join(here, 'Disruption_Results.html'), new_html_location)\n",
    "\n",
    "print('Finished: Report saved at: {}'.format(new_html_location))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "webbrowser.open('file://' + os.path.realpath(new_html_location))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
